package chat

import "github.com/rumpl/cagent/pkg/tools"

type ChatMessagePartType string

const (
	ChatMessagePartTypeText     ChatMessagePartType = "text"
	ChatMessagePartTypeImageURL ChatMessagePartType = "image_url"
)

type ImageURLDetail string

const (
	ImageURLDetailHigh ImageURLDetail = "high"
	ImageURLDetailLow  ImageURLDetail = "low"
	ImageURLDetailAuto ImageURLDetail = "auto"
)

type ChatMessageImageURL struct {
	URL    string         `json:"url,omitempty"`
	Detail ImageURLDetail `json:"detail,omitempty"`
}

type ChatCompletionMessage struct {
	Role         string `json:"role"`
	Content      string `json:"content,omitempty"`
	Refusal      string `json:"refusal,omitempty"`
	MultiContent []ChatMessagePart

	// This property isn't in the official documentation, but it's in
	// the documentation for the official library for python:
	// - https://github.com/openai/openai-python/blob/main/chatml.md
	// - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
	Name string `json:"name,omitempty"`

	// This property is used for the "reasoning" feature supported by deepseek-reasoner
	// which is not in the official documentation.
	// the doc from deepseek:
	// - https://api-docs.deepseek.com/api/create-chat-completion#responses
	ReasoningContent string `json:"reasoning_content,omitempty"`

	FunctionCall *tools.FunctionCall `json:"function_call,omitempty"`

	// For Role=assistant prompts this may be set to the tool calls generated by the model, such as function calls.
	ToolCalls []tools.ToolCall `json:"tool_calls,omitempty"`

	// For Role=tool prompts this should be set to the ID given in the assistant's prior request to call a tool.
	ToolCallID string `json:"tool_call_id,omitempty"`
}

type ChatMessagePart struct {
	Type     ChatMessagePartType  `json:"type,omitempty"`
	Text     string               `json:"text,omitempty"`
	ImageURL *ChatMessageImageURL `json:"image_url,omitempty"`
}

// FinishReason represents the reason why the model finished generating a response
type FinishReason string

const (
	// FinishReasonStop means the model reached a natural stopping point or the max tokens
	FinishReasonStop FinishReason = "stop"
	// FinishReasonLength means the model reached the token limit
	FinishReasonLength FinishReason = "length"
	// FinishReasonToolCalls means the model called a tool
	FinishReasonToolCalls FinishReason = "tool_calls"
	// FinishReasonFunctionCall is used when the model calls a function (legacy)
	FinishReasonFunctionCall FinishReason = "function_call"
	// FinishReasonContentFilter means the content was filtered
	FinishReasonContentFilter FinishReason = "content_filter"
	// FinishReasonNull means no finish reason was provided
	FinishReasonNull FinishReason = "null"
)

// ChatCompletionDelta represents a delta/chunk in a streaming response
type ChatCompletionDelta struct {
	Role         string              `json:"role,omitempty"`
	Content      string              `json:"content,omitempty"`
	FunctionCall *tools.FunctionCall `json:"function_call,omitempty"`
	ToolCalls    []tools.ToolCall    `json:"tool_calls,omitempty"`
}

// ChatCompletionStreamChoice represents a choice in a streaming response
type ChatCompletionStreamChoice struct {
	Index        int                 `json:"index"`
	Delta        ChatCompletionDelta `json:"delta"`
	FinishReason FinishReason        `json:"finish_reason,omitempty"`
}

// ChatCompletionStreamResponse represents a streaming response from the model
type ChatCompletionStreamResponse struct {
	ID      string                       `json:"id"`
	Object  string                       `json:"object"`
	Created int64                        `json:"created"`
	Model   string                       `json:"model"`
	Choices []ChatCompletionStreamChoice `json:"choices"`
}

// ChatCompletionStream interface represents a stream of chat completions
type ChatCompletionStream interface {
	// Recv gets the next completion chunk
	Recv() (ChatCompletionStreamResponse, error)
	// Close closes the stream
	Close()
}
